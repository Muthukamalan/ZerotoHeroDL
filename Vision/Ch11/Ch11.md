# CLASS ACTIVATION MAPS ðŸŽ“
CAMs are visualizations that highlight which regions of an input image contributes the most to the classification decision made by a deep NN.
[Advance Explainable NN](https://jacobgil.github.io/pytorch-gradcam-book/introduction.html)

<img src="../assets/Guided Gram-CAM.jpg" alt="Grad-CAM" width=600/>

**STEPS**
- load trained model
- load input image wants to analyse
- run with forward pass get final class classifier
- compute gradients of the final class classifier w.r.t feature maps
- average along the height and weight dimension (spatially)
- multiply with feature maps to get weighted feature maps
- normalize the activation map to get final Grad-CAM heatmap


# ADVERSARIAL IMAGE ðŸŽ­
[Example Adversarial Image](https://github.com/akshaychawla/Adversarial-Examples-in-PyTorch)

1. Choose an input image and its label: Start with a clean image, labeled as a specific class, which you want the network to misclassify.
2. Define the loss function: The loss function should measure the difference between the predicted class and the target class, usually the opposite of the true class.
3. Calculate the gradient of the loss with respect to the input image: This step is done using backpropagation and it gives information on how the loss changes as the input image are changed.
4. Perturb the input image: Add a small, but targeted perturbation to the input image, proportional to the gradient calculated in the previous step. This is done to maximize the loss and hence, change the predicted class.
5. Repeat steps 3 and 4 until the desired misclassification is achieved: You may need to repeat these steps multiple times to generate a strong enough perturbation that causes the network to misclassify the image.
6. Clip the pixel values of the perturbed image if needed: Clipping ensures that the perturbed image remains visually similar to the original image and falls within a specified range of pixel values.

<img src="../assets/adversarial-img.png" width=600 alt="adversarial image"/>


# LEARNING RATE ðŸ“š
## SGD (constant)
- gradient pertubation
- momentum
## ADAM (Adaptive)
- suitable for sparse data
- different learning parameter for different neurons


Don't decay the learning rate, increase the batch size!  (completely different approach)