# OPTIMIZERS
https://www.kaggle.com/code/isbhargav/guide-to-pytorch-learning-rate-scheduling


# CAM
- gradcam

# LR
## SGD (constant)
- gradient pertubation
- momentum
## ADAM (Adaptive)

Don't decay the learning rate, increase the batch size!  (completely different approach)