{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "import lightning as pl \n",
    "from lightning.pytorch.callbacks import (\n",
    "    ModelSummary,\n",
    "    ModelCheckpoint,\n",
    "    LearningRateMonitor,\n",
    "    LearningRateFinder\n",
    ")\n",
    "from lightning.pytorch.tuner import Tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.yolov3 import YOLOV3\n",
    "from src.loss import YOLOLoss\n",
    "from src.data import PASCALDataModule\n",
    "from src.config import (\n",
    "    DEVICE,\n",
    "    PASCAL_CLASSES,\n",
    "    NUM_CLASSES,\n",
    "    NUM_EPOCHS,\n",
    "    NUM_WORKERS,\n",
    "    LEARNING_RATE,\n",
    "    WEIGHT_DECAY,\n",
    "    SAVE_MODEL,\n",
    "    LOAD_MODEL,\n",
    "    IMAGE_DIR,\n",
    "    IMAGE_SIZE,\n",
    "    PIN_MEMORY,\n",
    "    DATASET,\n",
    "    BATCH_SIZE,\n",
    "    LABEL_DIR,\n",
    "    SHUFFLE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size:int = BATCH_SIZE\n",
    "shuffle:bool   = SHUFFLE\n",
    "num_workers:int= NUM_WORKERS\n",
    "learing_rate:float = LEARNING_RATE\n",
    "epochs:int     = 3\n",
    "num_classes:int = NUM_CLASSES\n",
    "num_devices    = os.cpu_count()-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = PASCALDataModule(\n",
    "    train_csv_path= str(os.path.join('..','..','data','PASCAL_VOC','train.csv')),\n",
    "    test_csv_path =  str(os.path.join('..','..','data','PASCAL_VOC','test.csv')),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlr = LEARNING_RATE\n",
    "scheduler_steps = len(datamodule.train_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelSummary(max_depth=10),\n",
    "    ModelCheckpoint(\n",
    "        dirpath=\"model_checkpoints\",\n",
    "        filename=\"yolov3_{epoch}\",\n",
    "        monitor=\"train_loss\",\n",
    "        mode=\"min\",\n",
    "        save_last=True,\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLOV3(in_channel=3,\n",
    "    num_classes=num_classes,\n",
    "    epochs=6,\n",
    "    loss_fn=YOLOLoss,\n",
    "    data_module=datamodule,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    maxlr=maxlr,\n",
    "    scheduler_steps=scheduler_steps,\n",
    "    device_count=num_devices,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[0],\n",
    "    num_nodes=1,\n",
    "    max_epochs=6,\n",
    "    callbacks=callbacks,\n",
    "    precision=\"16-mixed\",\n",
    "    check_val_every_n_epoch=10,\n",
    "    num_sanity_val_steps=0,\n",
    "    enable_checkpointing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuner = Tuner(trainer)\n",
    "\n",
    "# lr_finder = tuner.lr_find(\n",
    "#     model=model, min_lr=1e-5, train_dataloaders=datamodule.train_dataloader()\n",
    "# )\n",
    "# maxlr = lr_finder.suggestion()\n",
    "# fig = lr_finder.plot(suggest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 4050 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "    | Name                      | Type            | Params\n",
      "----------------------------------------------------------------\n",
      "0   | layers                    | ModuleList      | 61.6 M\n",
      "1   | layers.0                  | CNNBlock        | 928   \n",
      "2   | layers.0.conv             | Conv2d          | 864   \n",
      "3   | layers.0.bn               | BatchNorm2d     | 64    \n",
      "4   | layers.0.relu             | LeakyReLU       | 0     \n",
      "5   | layers.1                  | CNNBlock        | 18.6 K\n",
      "6   | layers.1.conv             | Conv2d          | 18.4 K\n",
      "7   | layers.1.bn               | BatchNorm2d     | 128   \n",
      "8   | layers.1.relu             | LeakyReLU       | 0     \n",
      "9   | layers.2                  | ResidualBlock   | 20.7 K\n",
      "10  | layers.2.layers           | ModuleList      | 20.7 K\n",
      "11  | layers.2.layers.0         | Sequential      | 20.7 K\n",
      "12  | layers.2.layers.0.0       | CNNBlock        | 2.1 K \n",
      "13  | layers.2.layers.0.0.conv  | Conv2d          | 2.0 K \n",
      "14  | layers.2.layers.0.0.bn    | BatchNorm2d     | 64    \n",
      "15  | layers.2.layers.0.0.relu  | LeakyReLU       | 0     \n",
      "16  | layers.2.layers.0.1       | CNNBlock        | 18.6 K\n",
      "17  | layers.2.layers.0.1.conv  | Conv2d          | 18.4 K\n",
      "18  | layers.2.layers.0.1.bn    | BatchNorm2d     | 128   \n",
      "19  | layers.2.layers.0.1.relu  | LeakyReLU       | 0     \n",
      "20  | layers.3                  | CNNBlock        | 74.0 K\n",
      "21  | layers.3.conv             | Conv2d          | 73.7 K\n",
      "22  | layers.3.bn               | BatchNorm2d     | 256   \n",
      "23  | layers.3.relu             | LeakyReLU       | 0     \n",
      "24  | layers.4                  | ResidualBlock   | 164 K \n",
      "25  | layers.4.layers           | ModuleList      | 164 K \n",
      "26  | layers.4.layers.0         | Sequential      | 82.3 K\n",
      "27  | layers.4.layers.0.0       | CNNBlock        | 8.3 K \n",
      "28  | layers.4.layers.0.0.conv  | Conv2d          | 8.2 K \n",
      "29  | layers.4.layers.0.0.bn    | BatchNorm2d     | 128   \n",
      "30  | layers.4.layers.0.0.relu  | LeakyReLU       | 0     \n",
      "31  | layers.4.layers.0.1       | CNNBlock        | 74.0 K\n",
      "32  | layers.4.layers.0.1.conv  | Conv2d          | 73.7 K\n",
      "33  | layers.4.layers.0.1.bn    | BatchNorm2d     | 256   \n",
      "34  | layers.4.layers.0.1.relu  | LeakyReLU       | 0     \n",
      "35  | layers.4.layers.1         | Sequential      | 82.3 K\n",
      "36  | layers.4.layers.1.0       | CNNBlock        | 8.3 K \n",
      "37  | layers.4.layers.1.0.conv  | Conv2d          | 8.2 K \n",
      "38  | layers.4.layers.1.0.bn    | BatchNorm2d     | 128   \n",
      "39  | layers.4.layers.1.0.relu  | LeakyReLU       | 0     \n",
      "40  | layers.4.layers.1.1       | CNNBlock        | 74.0 K\n",
      "41  | layers.4.layers.1.1.conv  | Conv2d          | 73.7 K\n",
      "42  | layers.4.layers.1.1.bn    | BatchNorm2d     | 256   \n",
      "43  | layers.4.layers.1.1.relu  | LeakyReLU       | 0     \n",
      "44  | layers.5                  | CNNBlock        | 295 K \n",
      "45  | layers.5.conv             | Conv2d          | 294 K \n",
      "46  | layers.5.bn               | BatchNorm2d     | 512   \n",
      "47  | layers.5.relu             | LeakyReLU       | 0     \n",
      "48  | layers.6                  | ResidualBlock   | 2.6 M \n",
      "49  | layers.6.layers           | ModuleList      | 2.6 M \n",
      "50  | layers.6.layers.0         | Sequential      | 328 K \n",
      "51  | layers.6.layers.0.0       | CNNBlock        | 33.0 K\n",
      "52  | layers.6.layers.0.0.conv  | Conv2d          | 32.8 K\n",
      "53  | layers.6.layers.0.0.bn    | BatchNorm2d     | 256   \n",
      "54  | layers.6.layers.0.0.relu  | LeakyReLU       | 0     \n",
      "55  | layers.6.layers.0.1       | CNNBlock        | 295 K \n",
      "56  | layers.6.layers.0.1.conv  | Conv2d          | 294 K \n",
      "57  | layers.6.layers.0.1.bn    | BatchNorm2d     | 512   \n",
      "58  | layers.6.layers.0.1.relu  | LeakyReLU       | 0     \n",
      "59  | layers.6.layers.1         | Sequential      | 328 K \n",
      "60  | layers.6.layers.1.0       | CNNBlock        | 33.0 K\n",
      "61  | layers.6.layers.1.0.conv  | Conv2d          | 32.8 K\n",
      "62  | layers.6.layers.1.0.bn    | BatchNorm2d     | 256   \n",
      "63  | layers.6.layers.1.0.relu  | LeakyReLU       | 0     \n",
      "64  | layers.6.layers.1.1       | CNNBlock        | 295 K \n",
      "65  | layers.6.layers.1.1.conv  | Conv2d          | 294 K \n",
      "66  | layers.6.layers.1.1.bn    | BatchNorm2d     | 512   \n",
      "67  | layers.6.layers.1.1.relu  | LeakyReLU       | 0     \n",
      "68  | layers.6.layers.2         | Sequential      | 328 K \n",
      "69  | layers.6.layers.2.0       | CNNBlock        | 33.0 K\n",
      "70  | layers.6.layers.2.0.conv  | Conv2d          | 32.8 K\n",
      "71  | layers.6.layers.2.0.bn    | BatchNorm2d     | 256   \n",
      "72  | layers.6.layers.2.0.relu  | LeakyReLU       | 0     \n",
      "73  | layers.6.layers.2.1       | CNNBlock        | 295 K \n",
      "74  | layers.6.layers.2.1.conv  | Conv2d          | 294 K \n",
      "75  | layers.6.layers.2.1.bn    | BatchNorm2d     | 512   \n",
      "76  | layers.6.layers.2.1.relu  | LeakyReLU       | 0     \n",
      "77  | layers.6.layers.3         | Sequential      | 328 K \n",
      "78  | layers.6.layers.3.0       | CNNBlock        | 33.0 K\n",
      "79  | layers.6.layers.3.0.conv  | Conv2d          | 32.8 K\n",
      "80  | layers.6.layers.3.0.bn    | BatchNorm2d     | 256   \n",
      "81  | layers.6.layers.3.0.relu  | LeakyReLU       | 0     \n",
      "82  | layers.6.layers.3.1       | CNNBlock        | 295 K \n",
      "83  | layers.6.layers.3.1.conv  | Conv2d          | 294 K \n",
      "84  | layers.6.layers.3.1.bn    | BatchNorm2d     | 512   \n",
      "85  | layers.6.layers.3.1.relu  | LeakyReLU       | 0     \n",
      "86  | layers.6.layers.4         | Sequential      | 328 K \n",
      "87  | layers.6.layers.4.0       | CNNBlock        | 33.0 K\n",
      "88  | layers.6.layers.4.0.conv  | Conv2d          | 32.8 K\n",
      "89  | layers.6.layers.4.0.bn    | BatchNorm2d     | 256   \n",
      "90  | layers.6.layers.4.0.relu  | LeakyReLU       | 0     \n",
      "91  | layers.6.layers.4.1       | CNNBlock        | 295 K \n",
      "92  | layers.6.layers.4.1.conv  | Conv2d          | 294 K \n",
      "93  | layers.6.layers.4.1.bn    | BatchNorm2d     | 512   \n",
      "94  | layers.6.layers.4.1.relu  | LeakyReLU       | 0     \n",
      "95  | layers.6.layers.5         | Sequential      | 328 K \n",
      "96  | layers.6.layers.5.0       | CNNBlock        | 33.0 K\n",
      "97  | layers.6.layers.5.0.conv  | Conv2d          | 32.8 K\n",
      "98  | layers.6.layers.5.0.bn    | BatchNorm2d     | 256   \n",
      "99  | layers.6.layers.5.0.relu  | LeakyReLU       | 0     \n",
      "100 | layers.6.layers.5.1       | CNNBlock        | 295 K \n",
      "101 | layers.6.layers.5.1.conv  | Conv2d          | 294 K \n",
      "102 | layers.6.layers.5.1.bn    | BatchNorm2d     | 512   \n",
      "103 | layers.6.layers.5.1.relu  | LeakyReLU       | 0     \n",
      "104 | layers.6.layers.6         | Sequential      | 328 K \n",
      "105 | layers.6.layers.6.0       | CNNBlock        | 33.0 K\n",
      "106 | layers.6.layers.6.0.conv  | Conv2d          | 32.8 K\n",
      "107 | layers.6.layers.6.0.bn    | BatchNorm2d     | 256   \n",
      "108 | layers.6.layers.6.0.relu  | LeakyReLU       | 0     \n",
      "109 | layers.6.layers.6.1       | CNNBlock        | 295 K \n",
      "110 | layers.6.layers.6.1.conv  | Conv2d          | 294 K \n",
      "111 | layers.6.layers.6.1.bn    | BatchNorm2d     | 512   \n",
      "112 | layers.6.layers.6.1.relu  | LeakyReLU       | 0     \n",
      "113 | layers.6.layers.7         | Sequential      | 328 K \n",
      "114 | layers.6.layers.7.0       | CNNBlock        | 33.0 K\n",
      "115 | layers.6.layers.7.0.conv  | Conv2d          | 32.8 K\n",
      "116 | layers.6.layers.7.0.bn    | BatchNorm2d     | 256   \n",
      "117 | layers.6.layers.7.0.relu  | LeakyReLU       | 0     \n",
      "118 | layers.6.layers.7.1       | CNNBlock        | 295 K \n",
      "119 | layers.6.layers.7.1.conv  | Conv2d          | 294 K \n",
      "120 | layers.6.layers.7.1.bn    | BatchNorm2d     | 512   \n",
      "121 | layers.6.layers.7.1.relu  | LeakyReLU       | 0     \n",
      "122 | layers.7                  | CNNBlock        | 1.2 M \n",
      "123 | layers.7.conv             | Conv2d          | 1.2 M \n",
      "124 | layers.7.bn               | BatchNorm2d     | 1.0 K \n",
      "125 | layers.7.relu             | LeakyReLU       | 0     \n",
      "126 | layers.8                  | ResidualBlock   | 10.5 M\n",
      "127 | layers.8.layers           | ModuleList      | 10.5 M\n",
      "128 | layers.8.layers.0         | Sequential      | 1.3 M \n",
      "129 | layers.8.layers.0.0       | CNNBlock        | 131 K \n",
      "130 | layers.8.layers.0.0.conv  | Conv2d          | 131 K \n",
      "131 | layers.8.layers.0.0.bn    | BatchNorm2d     | 512   \n",
      "132 | layers.8.layers.0.0.relu  | LeakyReLU       | 0     \n",
      "133 | layers.8.layers.0.1       | CNNBlock        | 1.2 M \n",
      "134 | layers.8.layers.0.1.conv  | Conv2d          | 1.2 M \n",
      "135 | layers.8.layers.0.1.bn    | BatchNorm2d     | 1.0 K \n",
      "136 | layers.8.layers.0.1.relu  | LeakyReLU       | 0     \n",
      "137 | layers.8.layers.1         | Sequential      | 1.3 M \n",
      "138 | layers.8.layers.1.0       | CNNBlock        | 131 K \n",
      "139 | layers.8.layers.1.0.conv  | Conv2d          | 131 K \n",
      "140 | layers.8.layers.1.0.bn    | BatchNorm2d     | 512   \n",
      "141 | layers.8.layers.1.0.relu  | LeakyReLU       | 0     \n",
      "142 | layers.8.layers.1.1       | CNNBlock        | 1.2 M \n",
      "143 | layers.8.layers.1.1.conv  | Conv2d          | 1.2 M \n",
      "144 | layers.8.layers.1.1.bn    | BatchNorm2d     | 1.0 K \n",
      "145 | layers.8.layers.1.1.relu  | LeakyReLU       | 0     \n",
      "146 | layers.8.layers.2         | Sequential      | 1.3 M \n",
      "147 | layers.8.layers.2.0       | CNNBlock        | 131 K \n",
      "148 | layers.8.layers.2.0.conv  | Conv2d          | 131 K \n",
      "149 | layers.8.layers.2.0.bn    | BatchNorm2d     | 512   \n",
      "150 | layers.8.layers.2.0.relu  | LeakyReLU       | 0     \n",
      "151 | layers.8.layers.2.1       | CNNBlock        | 1.2 M \n",
      "152 | layers.8.layers.2.1.conv  | Conv2d          | 1.2 M \n",
      "153 | layers.8.layers.2.1.bn    | BatchNorm2d     | 1.0 K \n",
      "154 | layers.8.layers.2.1.relu  | LeakyReLU       | 0     \n",
      "155 | layers.8.layers.3         | Sequential      | 1.3 M \n",
      "156 | layers.8.layers.3.0       | CNNBlock        | 131 K \n",
      "157 | layers.8.layers.3.0.conv  | Conv2d          | 131 K \n",
      "158 | layers.8.layers.3.0.bn    | BatchNorm2d     | 512   \n",
      "159 | layers.8.layers.3.0.relu  | LeakyReLU       | 0     \n",
      "160 | layers.8.layers.3.1       | CNNBlock        | 1.2 M \n",
      "161 | layers.8.layers.3.1.conv  | Conv2d          | 1.2 M \n",
      "162 | layers.8.layers.3.1.bn    | BatchNorm2d     | 1.0 K \n",
      "163 | layers.8.layers.3.1.relu  | LeakyReLU       | 0     \n",
      "164 | layers.8.layers.4         | Sequential      | 1.3 M \n",
      "165 | layers.8.layers.4.0       | CNNBlock        | 131 K \n",
      "166 | layers.8.layers.4.0.conv  | Conv2d          | 131 K \n",
      "167 | layers.8.layers.4.0.bn    | BatchNorm2d     | 512   \n",
      "168 | layers.8.layers.4.0.relu  | LeakyReLU       | 0     \n",
      "169 | layers.8.layers.4.1       | CNNBlock        | 1.2 M \n",
      "170 | layers.8.layers.4.1.conv  | Conv2d          | 1.2 M \n",
      "171 | layers.8.layers.4.1.bn    | BatchNorm2d     | 1.0 K \n",
      "172 | layers.8.layers.4.1.relu  | LeakyReLU       | 0     \n",
      "173 | layers.8.layers.5         | Sequential      | 1.3 M \n",
      "174 | layers.8.layers.5.0       | CNNBlock        | 131 K \n",
      "175 | layers.8.layers.5.0.conv  | Conv2d          | 131 K \n",
      "176 | layers.8.layers.5.0.bn    | BatchNorm2d     | 512   \n",
      "177 | layers.8.layers.5.0.relu  | LeakyReLU       | 0     \n",
      "178 | layers.8.layers.5.1       | CNNBlock        | 1.2 M \n",
      "179 | layers.8.layers.5.1.conv  | Conv2d          | 1.2 M \n",
      "180 | layers.8.layers.5.1.bn    | BatchNorm2d     | 1.0 K \n",
      "181 | layers.8.layers.5.1.relu  | LeakyReLU       | 0     \n",
      "182 | layers.8.layers.6         | Sequential      | 1.3 M \n",
      "183 | layers.8.layers.6.0       | CNNBlock        | 131 K \n",
      "184 | layers.8.layers.6.0.conv  | Conv2d          | 131 K \n",
      "185 | layers.8.layers.6.0.bn    | BatchNorm2d     | 512   \n",
      "186 | layers.8.layers.6.0.relu  | LeakyReLU       | 0     \n",
      "187 | layers.8.layers.6.1       | CNNBlock        | 1.2 M \n",
      "188 | layers.8.layers.6.1.conv  | Conv2d          | 1.2 M \n",
      "189 | layers.8.layers.6.1.bn    | BatchNorm2d     | 1.0 K \n",
      "190 | layers.8.layers.6.1.relu  | LeakyReLU       | 0     \n",
      "191 | layers.8.layers.7         | Sequential      | 1.3 M \n",
      "192 | layers.8.layers.7.0       | CNNBlock        | 131 K \n",
      "193 | layers.8.layers.7.0.conv  | Conv2d          | 131 K \n",
      "194 | layers.8.layers.7.0.bn    | BatchNorm2d     | 512   \n",
      "195 | layers.8.layers.7.0.relu  | LeakyReLU       | 0     \n",
      "196 | layers.8.layers.7.1       | CNNBlock        | 1.2 M \n",
      "197 | layers.8.layers.7.1.conv  | Conv2d          | 1.2 M \n",
      "198 | layers.8.layers.7.1.bn    | BatchNorm2d     | 1.0 K \n",
      "199 | layers.8.layers.7.1.relu  | LeakyReLU       | 0     \n",
      "200 | layers.9                  | CNNBlock        | 4.7 M \n",
      "201 | layers.9.conv             | Conv2d          | 4.7 M \n",
      "202 | layers.9.bn               | BatchNorm2d     | 2.0 K \n",
      "203 | layers.9.relu             | LeakyReLU       | 0     \n",
      "204 | layers.10                 | ResidualBlock   | 21.0 M\n",
      "205 | layers.10.layers          | ModuleList      | 21.0 M\n",
      "206 | layers.10.layers.0        | Sequential      | 5.2 M \n",
      "207 | layers.10.layers.0.0      | CNNBlock        | 525 K \n",
      "208 | layers.10.layers.0.0.conv | Conv2d          | 524 K \n",
      "209 | layers.10.layers.0.0.bn   | BatchNorm2d     | 1.0 K \n",
      "210 | layers.10.layers.0.0.relu | LeakyReLU       | 0     \n",
      "211 | layers.10.layers.0.1      | CNNBlock        | 4.7 M \n",
      "212 | layers.10.layers.0.1.conv | Conv2d          | 4.7 M \n",
      "213 | layers.10.layers.0.1.bn   | BatchNorm2d     | 2.0 K \n",
      "214 | layers.10.layers.0.1.relu | LeakyReLU       | 0     \n",
      "215 | layers.10.layers.1        | Sequential      | 5.2 M \n",
      "216 | layers.10.layers.1.0      | CNNBlock        | 525 K \n",
      "217 | layers.10.layers.1.0.conv | Conv2d          | 524 K \n",
      "218 | layers.10.layers.1.0.bn   | BatchNorm2d     | 1.0 K \n",
      "219 | layers.10.layers.1.0.relu | LeakyReLU       | 0     \n",
      "220 | layers.10.layers.1.1      | CNNBlock        | 4.7 M \n",
      "221 | layers.10.layers.1.1.conv | Conv2d          | 4.7 M \n",
      "222 | layers.10.layers.1.1.bn   | BatchNorm2d     | 2.0 K \n",
      "223 | layers.10.layers.1.1.relu | LeakyReLU       | 0     \n",
      "224 | layers.10.layers.2        | Sequential      | 5.2 M \n",
      "225 | layers.10.layers.2.0      | CNNBlock        | 525 K \n",
      "226 | layers.10.layers.2.0.conv | Conv2d          | 524 K \n",
      "227 | layers.10.layers.2.0.bn   | BatchNorm2d     | 1.0 K \n",
      "228 | layers.10.layers.2.0.relu | LeakyReLU       | 0     \n",
      "229 | layers.10.layers.2.1      | CNNBlock        | 4.7 M \n",
      "230 | layers.10.layers.2.1.conv | Conv2d          | 4.7 M \n",
      "231 | layers.10.layers.2.1.bn   | BatchNorm2d     | 2.0 K \n",
      "232 | layers.10.layers.2.1.relu | LeakyReLU       | 0     \n",
      "233 | layers.10.layers.3        | Sequential      | 5.2 M \n",
      "234 | layers.10.layers.3.0      | CNNBlock        | 525 K \n",
      "235 | layers.10.layers.3.0.conv | Conv2d          | 524 K \n",
      "236 | layers.10.layers.3.0.bn   | BatchNorm2d     | 1.0 K \n",
      "237 | layers.10.layers.3.0.relu | LeakyReLU       | 0     \n",
      "238 | layers.10.layers.3.1      | CNNBlock        | 4.7 M \n",
      "239 | layers.10.layers.3.1.conv | Conv2d          | 4.7 M \n",
      "240 | layers.10.layers.3.1.bn   | BatchNorm2d     | 2.0 K \n",
      "241 | layers.10.layers.3.1.relu | LeakyReLU       | 0     \n",
      "242 | layers.11                 | CNNBlock        | 525 K \n",
      "243 | layers.11.conv            | Conv2d          | 524 K \n",
      "244 | layers.11.bn              | BatchNorm2d     | 1.0 K \n",
      "245 | layers.11.relu            | LeakyReLU       | 0     \n",
      "246 | layers.12                 | CNNBlock        | 4.7 M \n",
      "247 | layers.12.conv            | Conv2d          | 4.7 M \n",
      "248 | layers.12.bn              | BatchNorm2d     | 2.0 K \n",
      "249 | layers.12.relu            | LeakyReLU       | 0     \n",
      "250 | layers.13                 | ResidualBlock   | 5.2 M \n",
      "251 | layers.13.layers          | ModuleList      | 5.2 M \n",
      "252 | layers.13.layers.0        | Sequential      | 5.2 M \n",
      "253 | layers.13.layers.0.0      | CNNBlock        | 525 K \n",
      "254 | layers.13.layers.0.0.conv | Conv2d          | 524 K \n",
      "255 | layers.13.layers.0.0.bn   | BatchNorm2d     | 1.0 K \n",
      "256 | layers.13.layers.0.0.relu | LeakyReLU       | 0     \n",
      "257 | layers.13.layers.0.1      | CNNBlock        | 4.7 M \n",
      "258 | layers.13.layers.0.1.conv | Conv2d          | 4.7 M \n",
      "259 | layers.13.layers.0.1.bn   | BatchNorm2d     | 2.0 K \n",
      "260 | layers.13.layers.0.1.relu | LeakyReLU       | 0     \n",
      "261 | layers.14                 | CNNBlock        | 525 K \n",
      "262 | layers.14.conv            | Conv2d          | 524 K \n",
      "263 | layers.14.bn              | BatchNorm2d     | 1.0 K \n",
      "264 | layers.14.relu            | LeakyReLU       | 0     \n",
      "265 | layers.15                 | ScalePrediction | 4.8 M \n",
      "266 | layers.15.pred            | Sequential      | 4.8 M \n",
      "267 | layers.15.pred.0          | CNNBlock        | 4.7 M \n",
      "268 | layers.15.pred.0.conv     | Conv2d          | 4.7 M \n",
      "269 | layers.15.pred.0.bn       | BatchNorm2d     | 2.0 K \n",
      "270 | layers.15.pred.0.relu     | LeakyReLU       | 0     \n",
      "271 | layers.15.pred.1          | CNNBlock        | 77.0 K\n",
      "272 | layers.15.pred.1.conv     | Conv2d          | 76.8 K\n",
      "273 | layers.15.pred.1.bn       | BatchNorm2d     | 150   \n",
      "274 | layers.15.pred.1.relu     | LeakyReLU       | 0     \n",
      "275 | layers.16                 | CNNBlock        | 131 K \n",
      "276 | layers.16.conv            | Conv2d          | 131 K \n",
      "277 | layers.16.bn              | BatchNorm2d     | 512   \n",
      "278 | layers.16.relu            | LeakyReLU       | 0     \n",
      "279 | layers.17                 | Upsample        | 0     \n",
      "280 | layers.18                 | CNNBlock        | 197 K \n",
      "281 | layers.18.conv            | Conv2d          | 196 K \n",
      "282 | layers.18.bn              | BatchNorm2d     | 512   \n",
      "283 | layers.18.relu            | LeakyReLU       | 0     \n",
      "284 | layers.19                 | CNNBlock        | 1.2 M \n",
      "285 | layers.19.conv            | Conv2d          | 1.2 M \n",
      "286 | layers.19.bn              | BatchNorm2d     | 1.0 K \n",
      "287 | layers.19.relu            | LeakyReLU       | 0     \n",
      "288 | layers.20                 | ResidualBlock   | 1.3 M \n",
      "289 | layers.20.layers          | ModuleList      | 1.3 M \n",
      "290 | layers.20.layers.0        | Sequential      | 1.3 M \n",
      "291 | layers.20.layers.0.0      | CNNBlock        | 131 K \n",
      "292 | layers.20.layers.0.0.conv | Conv2d          | 131 K \n",
      "293 | layers.20.layers.0.0.bn   | BatchNorm2d     | 512   \n",
      "294 | layers.20.layers.0.0.relu | LeakyReLU       | 0     \n",
      "295 | layers.20.layers.0.1      | CNNBlock        | 1.2 M \n",
      "296 | layers.20.layers.0.1.conv | Conv2d          | 1.2 M \n",
      "297 | layers.20.layers.0.1.bn   | BatchNorm2d     | 1.0 K \n",
      "298 | layers.20.layers.0.1.relu | LeakyReLU       | 0     \n",
      "299 | layers.21                 | CNNBlock        | 131 K \n",
      "300 | layers.21.conv            | Conv2d          | 131 K \n",
      "301 | layers.21.bn              | BatchNorm2d     | 512   \n",
      "302 | layers.21.relu            | LeakyReLU       | 0     \n",
      "303 | layers.22                 | ScalePrediction | 1.2 M \n",
      "304 | layers.22.pred            | Sequential      | 1.2 M \n",
      "305 | layers.22.pred.0          | CNNBlock        | 1.2 M \n",
      "306 | layers.22.pred.0.conv     | Conv2d          | 1.2 M \n",
      "307 | layers.22.pred.0.bn       | BatchNorm2d     | 1.0 K \n",
      "308 | layers.22.pred.0.relu     | LeakyReLU       | 0     \n",
      "309 | layers.22.pred.1          | CNNBlock        | 38.6 K\n",
      "310 | layers.22.pred.1.conv     | Conv2d          | 38.4 K\n",
      "311 | layers.22.pred.1.bn       | BatchNorm2d     | 150   \n",
      "312 | layers.22.pred.1.relu     | LeakyReLU       | 0     \n",
      "313 | layers.23                 | CNNBlock        | 33.0 K\n",
      "314 | layers.23.conv            | Conv2d          | 32.8 K\n",
      "315 | layers.23.bn              | BatchNorm2d     | 256   \n",
      "316 | layers.23.relu            | LeakyReLU       | 0     \n",
      "317 | layers.24                 | Upsample        | 0     \n",
      "318 | layers.25                 | CNNBlock        | 49.4 K\n",
      "319 | layers.25.conv            | Conv2d          | 49.2 K\n",
      "320 | layers.25.bn              | BatchNorm2d     | 256   \n",
      "321 | layers.25.relu            | LeakyReLU       | 0     \n",
      "322 | layers.26                 | CNNBlock        | 295 K \n",
      "323 | layers.26.conv            | Conv2d          | 294 K \n",
      "324 | layers.26.bn              | BatchNorm2d     | 512   \n",
      "325 | layers.26.relu            | LeakyReLU       | 0     \n",
      "326 | layers.27                 | ResidualBlock   | 328 K \n",
      "327 | layers.27.layers          | ModuleList      | 328 K \n",
      "328 | layers.27.layers.0        | Sequential      | 328 K \n",
      "329 | layers.27.layers.0.0      | CNNBlock        | 33.0 K\n",
      "330 | layers.27.layers.0.0.conv | Conv2d          | 32.8 K\n",
      "331 | layers.27.layers.0.0.bn   | BatchNorm2d     | 256   \n",
      "332 | layers.27.layers.0.0.relu | LeakyReLU       | 0     \n",
      "333 | layers.27.layers.0.1      | CNNBlock        | 295 K \n",
      "334 | layers.27.layers.0.1.conv | Conv2d          | 294 K \n",
      "335 | layers.27.layers.0.1.bn   | BatchNorm2d     | 512   \n",
      "336 | layers.27.layers.0.1.relu | LeakyReLU       | 0     \n",
      "337 | layers.28                 | CNNBlock        | 33.0 K\n",
      "338 | layers.28.conv            | Conv2d          | 32.8 K\n",
      "339 | layers.28.bn              | BatchNorm2d     | 256   \n",
      "340 | layers.28.relu            | LeakyReLU       | 0     \n",
      "341 | layers.29                 | ScalePrediction | 314 K \n",
      "342 | layers.29.pred            | Sequential      | 314 K \n",
      "343 | layers.29.pred.0          | CNNBlock        | 295 K \n",
      "344 | layers.29.pred.0.conv     | Conv2d          | 294 K \n",
      "345 | layers.29.pred.0.bn       | BatchNorm2d     | 512   \n",
      "346 | layers.29.pred.0.relu     | LeakyReLU       | 0     \n",
      "347 | layers.29.pred.1          | CNNBlock        | 19.4 K\n",
      "348 | layers.29.pred.1.conv     | Conv2d          | 19.2 K\n",
      "349 | layers.29.pred.1.bn       | BatchNorm2d     | 150   \n",
      "350 | layers.29.pred.1.relu     | LeakyReLU       | 0     \n",
      "----------------------------------------------------------------\n",
      "61.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "61.6 M    Total params\n",
      "246.505   Total estimated model params size (MB)\n",
      "c:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/65 [00:00<?, ?it/s] "
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in pin memory thread for device 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py\", line 36, in do_one_step\n    data = pin_memory(data, device)\n  File \"c:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py\", line 72, in pin_memory\n    return type(data)([pin_memory(sample, device) for sample in data])  # type: ignore[call-arg]\n  File \"c:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py\", line 72, in <listcomp>\n    return type(data)([pin_memory(sample, device) for sample in data])  # type: ignore[call-arg]\n  File \"c:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py\", line 72, in pin_memory\n    return type(data)([pin_memory(sample, device) for sample in data])  # type: ignore[call-arg]\n  File \"c:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py\", line 72, in <listcomp>\n    return type(data)([pin_memory(sample, device) for sample in data])  # type: ignore[call-arg]\n  File \"c:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py\", line 57, in pin_memory\n    return data.pin_memory(device)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0007079457843841378\u001b[39m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mmaxlr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0007079457843841378\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    576\u001b[0m     ckpt_path,\n\u001b[0;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    579\u001b[0m )\n\u001b[1;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:987\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 987\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    992\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1033\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1031\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[0;32m   1032\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1033\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:212\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m     batch, _, __ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# TODO: we should instead use the batch_idx returned by the fetcher, however, that will require saving the\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# fetcher state so that the batch_idx is correct after restarting\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     batch_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\lightning\\pytorch\\loops\\fetchers.py:133\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatches\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\lightning\\pytorch\\loops\\fetchers.py:60\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_profiler()\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\lightning\\pytorch\\utilities\\combined_loader.py:341\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _ITERATOR_RETURN:\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 341\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n\u001b[0;32m    343\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\lightning\\pytorch\\utilities\\combined_loader.py:78\u001b[0m, in \u001b[0;36m_MaxSizeCycle.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m         out[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consumed[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[1;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[0;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\torch\\_utils.py:722\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[0;32m    720\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 722\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Caught RuntimeError in pin memory thread for device 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py\", line 36, in do_one_step\n    data = pin_memory(data, device)\n  File \"c:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py\", line 72, in pin_memory\n    return type(data)([pin_memory(sample, device) for sample in data])  # type: ignore[call-arg]\n  File \"c:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py\", line 72, in <listcomp>\n    return type(data)([pin_memory(sample, device) for sample in data])  # type: ignore[call-arg]\n  File \"c:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py\", line 72, in pin_memory\n    return type(data)([pin_memory(sample, device) for sample in data])  # type: ignore[call-arg]\n  File \"c:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py\", line 72, in <listcomp>\n    return type(data)([pin_memory(sample, device) for sample in data])  # type: ignore[call-arg]\n  File \"c:\\Users\\muthu\\miniconda3\\envs\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py\", line 57, in pin_memory\n    return data.pin_memory(device)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n"
     ]
    }
   ],
   "source": [
    "model.learning_rate = 0.0007079457843841378\n",
    "model.maxlr = 0.0007079457843841378\n",
    "\n",
    "trainer.fit(model=model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isfile(\"../../../data/PASCAL_VOC//labels/2009_001854.txt\")\n",
    "os.path.isfile(\"../../data/PASCAL_VOC//labels/2009_001854.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
